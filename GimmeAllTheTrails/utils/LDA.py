from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt


# Create a function to build the optimal LDA model
def optimal_lda_model(df_review, review_colname):
    """
    INPUTS:
        df_review - dataframe that contains the reviews
        review_colname: name of column that contains reviews

    OUTPUTS:
        lda_tfidf - Latent Dirichlet Allocation (LDA) model
        dtm_tfidf - document-term matrix in the tfidf format
        tfidf_vectorizer - word frequency in the reviews
        A graph comparing LDA Model Performance Scores with different params
    """
    docs_raw = df_review[review_colname].tolist()

    # ************   Step 1: Convert to document-term matrix   ************#

    # Transform text to vector form using the vectorizer object
    tf_vectorizer = CountVectorizer(
        strip_accents="unicode",
        stop_words="english",
        lowercase=True,
        token_pattern=r"\b[a-zA-Z]{3,}\b",  # num chars > 3 to avoid some meaningless words
        max_df=0.9,  # discard words that appear in > 90% of the reviews
        min_df=10,
    )  # discard words that appear in < 10 reviews

    # apply transformation
    tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())

    # convert to document-term matrix
    dtm_tfidf = tfidf_vectorizer.fit_transform(docs_raw)

    print(
        "The shape of the tfidf is {}, meaning that there are {} {} and {} tokens made through the filtering process.".format(
            dtm_tfidf.shape, dtm_tfidf.shape[0], review_colname, dtm_tfidf.shape[1]
        )
    )

    # *******   Step 2: GridSearch & parameter tuning to find the optimal LDA model   *******#

    # Define Search Param
    search_params = {
        "n_components": [5, 10, 15, 20, 25, 30],
        "learning_decay": [0.5, 0.7, 0.9],
    }

    # Init the Model
    lda = LatentDirichletAllocation()

    # Init Grid Search Class
    model = GridSearchCV(lda, param_grid=search_params)

    # Do the Grid Search
    model.fit(dtm_tfidf)

    # *****  Step 3: Output the optimal lda model and its parameters  *****#

    # Best Model
    best_lda_model = model.best_estimator_

    # Model Parameters
    print("Best Model's Params: ", model.best_params_)

    # Log Likelihood Score: Higher the better
    print("Model Log Likelihood Score: ", model.best_score_)

    # Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)
    print("Model Perplexity: ", best_lda_model.perplexity(dtm_tfidf))

    # ***********   Step 4: Compare LDA Model Performance Scores   ***********#

    # Get Log Likelyhoods from Grid Search Output
    gscore = model.fit(dtm_tfidf).cv_results_
    n_topics = [5, 10, 15, 20, 25, 30]

    log_likelyhoods_5 = [
        gscore["mean_test_score"][gscore["params"].index(v)]
        for v in gscore["params"]
        if v["learning_decay"] == 0.5
    ]
    log_likelyhoods_7 = [
        gscore["mean_test_score"][gscore["params"].index(v)]
        for v in gscore["params"]
        if v["learning_decay"] == 0.7
    ]
    log_likelyhoods_9 = [
        gscore["mean_test_score"][gscore["params"].index(v)]
        for v in gscore["params"]
        if v["learning_decay"] == 0.9
    ]

    # Show graph
    plt.figure(figsize=(12, 8))
    plt.plot(n_topics, log_likelyhoods_5, label="0.5")
    plt.plot(n_topics, log_likelyhoods_7, label="0.7")
    plt.plot(n_topics, log_likelyhoods_9, label="0.9")
    plt.title("Choosing Optimal LDA Model")
    plt.xlabel("Num Topics")
    plt.ylabel("Log Likelyhood Scores")
    plt.legend(title="Learning decay", loc="best")
    plt.show()

    return best_lda_model, dtm_tfidf, tfidf_vectorizer
